# Generated Image Classification

## Overview
This project involves training deep classification models on a dataset containing images generated by deep generative models. The goal is to predict the class label for each test image, with the performance evaluated using the macro F1 score. The project was developed as part of a deep learning course.

## Project Structure

### 1. Introduction
- **Objective**: Train and evaluate deep learning models to classify images generated by deep generative models.
- **Approach**: Implemented a simple Multi-Layer Perceptron (MLP) and a Convolutional Neural Network (CNN) based on a scaled-down ResNet34 architecture.

### 2. Data Preprocessing and Augmentation
- **Dataset**: 
  - 20,000 images provided.
  - Training set: 13,000 images.
  - Validation set: 2,000 images.
  - Test set: 5,000 images (labels not provided).
- **Preprocessing**: 
  - Images were loaded and shuffled to introduce randomness.
  - Normalization of images and one-hot encoding of labels.
- **Augmentation**: 
  - Employed `ImageDataGenerator` from TensorFlow for random rotations, shifts, zooms, and horizontal flips to prevent overfitting and improve the F1 score on the validation set.

### 3. Model Architectures
- **Multi-Layer Perceptron (MLP)**:
  - **Architecture**:
    - Flatten layer.
    - Fully connected layers with 24, 48, and 100 neurons respectively.
    - Dropout layer for regularization.
    - Softmax activation function in the output layer.
  - **Performance**: The MLP model performed poorly with 0% precision and recall.

- **Convolutional Neural Network (CNN)**:
  - **Architecture**:
    - Input layer.
    - 7x7 convolution layer followed by batch normalization and ReLU activation.
    - Pooling layer.
    - ResNet blocks with 3x3 convolutions, batch normalization, and skip connections.
    - Fully connected layers and additional dropout for regularization.
  - **Performance**: The CNN model achieved a promising F1 score of around 30% on the validation set after 20 epochs.

### 4. Hyperparameter Tuning
- **MLP Hyperparameter Tuning**:
  - Grid search over the number of neurons, learning rate, and optimizer.
  - **Results**: Validation loss was locked at around 5, indicating that the architecture was limiting the model's performance.

- **CNN Hyperparameter Tuning**:
  - Added a fully connected layer after each ResNet block.
  - Grid search over the number of neurons, learning rate, and optimizer.
  - **Results**: 
    - The learning rate and optimizer influenced the validation loss.
    - SGD provided the most consistent performance, with validation loss stabilizing around 4.6.
    - The final model included a fully connected layer after the ResNet blocks and a dropout layer for regularization.

### 5. Results
- **CNN Performance**: 
  - Achieved a validation F1 score of approximately 47% after tuning and training for 20 epochs.
  - Test data F1 score was around 50%.
  - Some classes were poorly predicted, indicating room for improvement.
- **MLP Performance**: 
  - The MLP model plateaued at an F1 score of 21% after 60 epochs, demonstrating poor performance.

### 6. Conclusion
- The CNN model outperformed the MLP, achieving an F1 score of 50% on the test data. However, there is significant potential for improvement by altering the model architecture or experimenting with different data augmentation techniques.

### 7. Bibliography
- [Understanding and Visualizing ResNets](https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8)

## How to Run
1. **Clone the Repository**:
    git clone https://github.com/madalin312/deep_generated_images_classification
	
2. **Install Dependencies**:
    - Ensure you have Python and the necessary libraries installed:

3. **Run the Project**:
    - Open the Jupyter Notebook provided or run the scripts to train the models and evaluate their performance.

4. **Evaluate the Results**:
    - The results, including confusion matrices and F1 scores, are generated as part of the model training process.

## Author
- **Radu Madalin-Cristian**, Group 507
